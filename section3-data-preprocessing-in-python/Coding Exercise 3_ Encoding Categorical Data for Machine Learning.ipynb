{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSwkz4Voa+gOqEJZfk8+l+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Coding Exercise 3: Encoding Categorical Data for Machine Learning\n","\n","1: Import required libraries - Pandas, Numpy, and required classes for this task - ColumnTransformer, OneHotEncoder, LabelEncoder.\n","\n","2: Start by loading the Titanic dataset into a pandas data frame. This can be done using the pd.read_csv function. The dataset's name is 'titanic.csv'.\n","\n","3: Identify the categorical features in your dataset that need to be encoded. You can store these feature names in a list for easy access later.\n","\n","4: To apply OneHotEncoding to these categorical features, create an instance of the ColumnTransformer class. Make sure to pass the OneHotEncoder() as an argument along with the list of categorical features.\n","\n","5: Use the fit_transform method on the instance of ColumnTransformer to apply the OneHotEncoding.\n","\n","6: The output of the fit_transform method should be converted into a NumPy array for further use.\n","\n","7: The 'Survived' column in your dataset is the dependent variable. This is a binary categorical variable that should be encoded using LabelEncoder.\n","\n","8: Print the updated matrix of features and the dependent variable vector"],"metadata":{"id":"fCByuwvstDbL"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLSjJdTOtxBa","executionInfo":{"status":"ok","timestamp":1731791411252,"user_tz":300,"elapsed":3911,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"199f689e-8a13-413c-e59f-28388fd0eeac"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEWcEKVgtCN5","executionInfo":{"status":"ok","timestamp":1731791499836,"user_tz":300,"elapsed":589,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"0c7677f6-68fc-4e5b-a36c-17c55d5b7d6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["   PassengerId  Survived  Pclass  \\\n","0            2         1       1   \n","1            4         1       1   \n","2            7         0       1   \n","3           11         1       3   \n","4           12         1       1   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","2                            McCarthy, Mr. Timothy J    male  54.0      0   \n","3                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n","4                           Bonnell, Miss. Elizabeth  female  58.0      0   \n","\n","   Parch    Ticket     Fare Cabin Embarked  \n","0      0  PC 17599  71.2833   C85        C  \n","1      0    113803  53.1000  C123        S  \n","2      0     17463  51.8625   E46        S  \n","3      1   PP 9549  16.7000    G6        S  \n","4      0    113783  26.5500  C103        S  \n"]}],"source":["# Importing the necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load the dataset\n","df = pd.read_csv('/content/drive/MyDrive/Udemy /Udemy - Kirill  - Machine Learning A-Z/Machine Learning A-Z/Part 1 - Data Preprocessing/Section 2 -------------------- Part 1 - Data Preprocessing --------------------/Python/titanic.csv')\n","print(df.head())\n","\n","# Split features and target\n","X = df.iloc[:, [0] + list(range(2, dataset.shape[1]))]\n","y = df.iloc[:, 1]\n"]},{"cell_type":"code","source":["print(X)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7gDExwfZwSOJ","executionInfo":{"status":"ok","timestamp":1731791414513,"user_tz":300,"elapsed":12,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"1936527f-a10b-4e5b-d46a-561e33632394"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["     PassengerId  Pclass                                               Name  \\\n","0              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n","1              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n","2              7       1                            McCarthy, Mr. Timothy J   \n","3             11       3                    Sandstrom, Miss. Marguerite Rut   \n","4             12       1                           Bonnell, Miss. Elizabeth   \n","..           ...     ...                                                ...   \n","178          872       1   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   \n","179          873       1                           Carlsson, Mr. Frans Olof   \n","180          880       1      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   \n","181          888       1                       Graham, Miss. Margaret Edith   \n","182          890       1                              Behr, Mr. Karl Howell   \n","\n","        Sex   Age  SibSp  Parch    Ticket     Fare        Cabin Embarked  \n","0    female  38.0      1      0  PC 17599  71.2833          C85        C  \n","1    female  35.0      1      0    113803  53.1000         C123        S  \n","2      male  54.0      0      0     17463  51.8625          E46        S  \n","3    female   4.0      1      1   PP 9549  16.7000           G6        S  \n","4    female  58.0      0      0    113783  26.5500         C103        S  \n","..      ...   ...    ...    ...       ...      ...          ...      ...  \n","178  female  47.0      1      1     11751  52.5542          D35        S  \n","179    male  33.0      0      0       695   5.0000  B51 B53 B55        S  \n","180  female  56.0      0      1     11767  83.1583          C50        C  \n","181  female  19.0      0      0    112053  30.0000          B42        S  \n","182    male  26.0      0      0    111369  30.0000         C148        C  \n","\n","[183 rows x 11 columns]\n"]}]},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sd1-4ok_x2FR","executionInfo":{"status":"ok","timestamp":1731791414513,"user_tz":300,"elapsed":11,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"93eabafe-b3e9-4689-8f7b-200fc0597c4b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0      1\n","1      1\n","2      0\n","3      1\n","4      1\n","      ..\n","178    1\n","179    0\n","180    1\n","181    1\n","182    1\n","Name: Survived, Length: 183, dtype: int64\n"]}]},{"cell_type":"code","source":["print(df.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tnQXPugwQwR","executionInfo":{"status":"ok","timestamp":1731791414513,"user_tz":300,"elapsed":10,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"546ffeca-a74d-40ab-e056-97adf894aa13"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["PassengerId      int64\n","Survived         int64\n","Pclass           int64\n","Name            object\n","Sex             object\n","Age            float64\n","SibSp            int64\n","Parch            int64\n","Ticket          object\n","Fare           float64\n","Cabin           object\n","Embarked        object\n","dtype: object\n"]}]},{"cell_type":"code","source":["print(df.isna().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjupLxBjyOJi","executionInfo":{"status":"ok","timestamp":1731791414513,"user_tz":300,"elapsed":8,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"f65ec9b4-1ca7-42f8-856b-4418e03410a6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["PassengerId    0\n","Survived       0\n","Pclass         0\n","Name           0\n","Sex            0\n","Age            0\n","SibSp          0\n","Parch          0\n","Ticket         0\n","Fare           0\n","Cabin          0\n","Embarked       0\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["### Encoding Categorical data"],"metadata":{"id":"-_7dxNwdyeYK"}},{"cell_type":"code","source":["# Identify the categorical data\n","categorical_features = ['Sex', 'Embarked', 'Pclass']\n","\n","# Implement an instance of the ColumnTransformer class\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n","\n","# Apply the fit_transform method on the instance of ColumnTransformer\n","X = ct.fit_transform(df)\n","\n","# Convert the output into a NumPy array\n","X = np.array(X)"],"metadata":{"id":"Sb3q2AeZxKp4","executionInfo":{"status":"ok","timestamp":1731791507676,"user_tz":300,"elapsed":639,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Encoding Dependent Variable"],"metadata":{"id":"WKRJ6lSmyhok"}},{"cell_type":"code","source":["# Use LabelEncoder to encode binary categorical data\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","\n","# Print the updated matrix of features and the dependent variable vector\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnW_W0C9xfwH","executionInfo":{"status":"ok","timestamp":1731791510667,"user_tz":300,"elapsed":517,"user":{"displayName":"Amara Chaw","userId":"15442814865005417977"}},"outputId":"225a6e23-7ca1-4b0e-9456-cbb8aaa18143"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 1 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1\n"," 1 1 1 0 1 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n"," 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 0 1 1\n"," 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1\n"," 0 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1]\n"]}]}]}